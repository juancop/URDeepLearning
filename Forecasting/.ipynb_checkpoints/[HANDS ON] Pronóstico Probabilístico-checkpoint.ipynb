{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "radio-socket",
   "metadata": {},
   "source": [
    "# Pronósticos Usando Deep Learning\n",
    "## ¡Un Pronóstico Probabilístico!\n",
    "\n",
    "¡Hola de nuevo! Anteriormente realizamos un ejercicio para pronosticar el consumo de energía eléctrica de 370 consumidores en USA. Nuestro problema sigue siendo el mismo, es decir:\n",
    "\n",
    "![context](imgs/context.png)\n",
    "\n",
    "Sin embargo, a lo largo de este ejercicio daremos un paso más allá. Y es que nuestro pronóstico no será únicamente un pronóstico puntual, sino que generaremos toda una distribución al rededor de sus posibles valores.\n",
    "\n",
    "Este ejercicio será guiado, pero dependerá de ti resolverlo y experimentar. \n",
    "\n",
    "Completa el código donde veas la indicación `### TU CÓDIGO AQUÍ ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-director",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
    "\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/juancop/URDeepLearning/blob/main/Forecasting/%5BHANDS%20ON%5D%20Pronóstico%20Probabilístico.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/juancop/URDeepLearning/blob/main/Forecasting/Pron%C3%B3stico%20Probabil%C3%ADstico.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  \n",
    "</table>\n",
    "\n",
    "Si estás en colab corre la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/juancop/URDeepLearning && mv URDeepLearning/Forecasting/sample ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__ # ¡Verifica nuevamente que estés usando la versión 2.3.1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-scottish",
   "metadata": {},
   "source": [
    "## Carga de Información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample/electricity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-society",
   "metadata": {},
   "source": [
    "# Datos para Entrenamiento\n",
    "\n",
    "En este punto estmaos ya familiarizados con la información que estamos trabajando, por lo que nos saltaremos la exploración de los datos. \n",
    "\n",
    "La generación de ventanas de entrenamiento es bastante importante, pues a partir de ellas el modelo aprende. En esta sección te encargarás de crear una función de procesamiento que te entregue la variable objetivo como la desees (ya sea con o sin logaritmo) y el conjunto de variables explicativas que quieras usar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "class processElectricityDataFrame:\n",
    "    \"\"\"\n",
    "    Procesa el DataFrame para generar un data set de ventanas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_df, val_df, test_df, index_variable, target_variable,\n",
    "                input_window, output_window, step = 24):\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df   \n",
    "        \n",
    "        self.label_name = target_variable\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "        self.index_variable = index_variable\n",
    "        self.step = step\n",
    "        \n",
    "        \n",
    "        self.model_columns = None \n",
    "            \n",
    "    \n",
    "    def create_npy_dataset(self, df, folder, name, columns, input_window, output_window, label_name):\n",
    "        \"\"\"\n",
    "            df: DataFrame a utilizar\n",
    "            folder: Folder para almacenar el .npy\n",
    "            columns: Lista de las columnas que se utilizarán al final\n",
    "            name: Nombre para el archivo (train, val, test)\n",
    "            input_window: Tamaño de la ventana de entrada (12)\n",
    "            output_window: Tramaño de la ventana de salida (12)\n",
    "            label_name: Nombre de la variable objetivo\n",
    "        \n",
    "        \"\"\"\n",
    "        file_input = os.path.join(folder, f'input_{name}.npy')\n",
    "        file_output = os.path.join(folder, f'output_{name}.npy')\n",
    "\n",
    "        unique_id_names = df[self.index_variable].value_counts().index.values\n",
    "        npy_input = None\n",
    "        npy_output = None\n",
    "\n",
    "        label_index = columns.index(label_name)\n",
    "        input_columns = columns.copy()\n",
    "            \n",
    "        window_size = input_window + output_window\n",
    "        \n",
    "        print(f'Started .npy creation with name: {name}')\n",
    "        for id_name in tqdm(unique_id_names):\n",
    "            \n",
    "            customer_df = df[df[self.index_variable] == id_name].sort_values(by = 'date').copy()\n",
    "            output_tmp_df = customer_df[columns].values\n",
    "            input_tmp_df = customer_df[input_columns].values\n",
    "            n_windows = len(output_tmp_df) - window_size + 1\n",
    "            \n",
    "            input_array = np.expand_dims(np.arange(self.input_window), 0) +  np.expand_dims(np.arange(n_windows, step = self.step), 0).T\n",
    "            output_array = np.expand_dims(np.arange(self.output_window), 0) + self.input_window +  np.expand_dims(np.arange(n_windows, step = self.step), 0).T\n",
    "            \n",
    "            \n",
    "            inputs = input_tmp_df[input_array].astype(np.float32)\n",
    "            labels = output_tmp_df[output_array, label_index].astype(np.float32)\n",
    "            \n",
    "            if npy_input is None:\n",
    "                \n",
    "                npy_input = inputs\n",
    "                npy_output = labels\n",
    "            else:\n",
    "                \n",
    "                npy_input = np.append(npy_input, inputs, axis = 0)\n",
    "                npy_output = np.append(npy_output, labels, axis = 0)\n",
    "                \n",
    "                random_index = np.random.choice(npy_input.shape[0], npy_input.shape[0], replace=False)\n",
    "                \n",
    "                npy_input = npy_input[random_index, :, :]\n",
    "                npy_output = npy_output[random_index, :]\n",
    "\n",
    "        with open(file_input, 'wb') as f:\n",
    "            np.save(f, npy_input)\n",
    "\n",
    "        with open(file_output, 'wb') as f:\n",
    "            np.save(f, npy_output)\n",
    "\n",
    "        print('Completed .npy creation.')        \n",
    "\n",
    "        \n",
    "    def preprocess_fn(self, df, model_type):      \n",
    "        if model_type == 'raw_inf':\n",
    "            df[self.label_name] = np.log(df[self.label_name] + 1)\n",
    "            self.model_columns = [self.label_name]#, 'hour', 'day_of_week', 'month']\n",
    "            return df\n",
    "        \n",
    "        \n",
    "        ### TU CÓDIGO AQUÍ ####\n",
    "        \n",
    "        # Define tu función de procesamiento e indica en self.model_columns las variables que utilizarás para entrenar\n",
    "        # OJO: La variable objetivo debe ser la primera en la lista.\n",
    "        \n",
    "        # Esta función debe retornar un data frame con todas las variables que definiste\n",
    "        \n",
    "        if model_type == \n",
    "        \n",
    "        ### TERMINA ###\n",
    "\n",
    "        \n",
    "    def create_all_datasets(self, folder_name = 'data', model_type = 'raw_inf'):\n",
    "        \n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "        \n",
    "        train_df = self.preprocess_fn(self.train_df, model_type)    \n",
    "         \n",
    "        self.create_npy_dataset(df = train_df, folder = folder_name, name = 'train', columns =  self.model_columns,\n",
    "                               input_window = self.input_window, output_window = self.output_window, label_name = self.label_name)\n",
    "        \n",
    "        val_df = self.preprocess_fn(self.val_df, model_type)    \n",
    "         \n",
    "        self.create_npy_dataset(df = val_df, folder = folder_name, name = 'val', columns =  self.model_columns,\n",
    "                               input_window = self.input_window, output_window = self.output_window, label_name = self.label_name)\n",
    "        \n",
    "        \n",
    "        test_df = self.preprocess_fn(self.test_df, model_type)    \n",
    "         \n",
    "        self.create_npy_dataset(df = test_df, folder = folder_name, name = 'test', columns =  self.model_columns,\n",
    "                               input_window = self.input_window, output_window = self.output_window, label_name = self.label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[df.date >= '2014-08-31']\n",
    "val_df = df[(df.date >= '2014-08-24') & (df.date < '2014-08-31')]\n",
    "train_df = df[df.date < '2014-08-24']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-wheat",
   "metadata": {},
   "source": [
    "A continuación generaremos los datos para el modelo. ¡Sí! Con las ventanas y la función de procesamiento que definiste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process = processElectricityDataFrame(train_df, \n",
    "                                         val_df, \n",
    "                                         test_df, \n",
    "                                         index_variable = 'categorical_id',\n",
    "                                         target_variable = 'power_usage',\n",
    "                                         input_window = 48,\n",
    "                                         output_window = 24,\n",
    "                                         step = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TU CÓDIGO AQUÍ ###\n",
    "#¿Cómo llamaste a tu función de procesamiento? \n",
    "df_process.create_all_datasets(folder_name = 'data', model_type = )\n",
    "\n",
    "### TERMINA ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-compound",
   "metadata": {},
   "source": [
    "# Pronóstico Probabilístico\n",
    "\n",
    "Hasta este punto no hemos hecho nada raro. ¿Cómo hacemos que nuestro modelo prediga un cuantil si ni siquiera conocemos la distribución subyacente a cada serie? La respuesta se encuentra en lo que se conoce como *Quantile Loss*, o también *Pinball Loss*.  ¿Te suena la métrica conocida como $\\operatorname{MAD}$? En el contexto de regressión lineal, si el criterio es minimizar la $\\operatorname{MAD}$ (Median Absolute Deviation), estaríamos haciendo una regresión sobre la *mediana* en lugar de la media. \n",
    "\n",
    "Esta función de pérdida viene dada por, \n",
    "\n",
    "$$QL(y, \\hat{y}, \\tau) = \\operatorname{max}(\\tau (y - \\hat{y}), (1 - \\tau)(\\hat{y} - y))$$\n",
    "\n",
    "A modo de ejemplo, si $\\tau = 0.75$, la función se vería como:\n",
    "\n",
    "$$QL(y, \\hat{y}, \\tau) = \\operatorname{max}(0.75 (y - \\hat{y}), 0.25(\\hat{y} - y))$$\n",
    "\n",
    "donde el modelo penalizaría más fuertemente una predicción por encima de $y$ que una por debajo. Lo que se busca es que los errores de predicción por encima sean apenas el 25% de las veces, mientras que por debajo sean el 75% de las veces. \n",
    "\n",
    "Dicho lo anterior, completa la función de pérdida para nuestro problema, asumiendo que `y_true` es un vector de dimensión `(N, T)` y `y_pred`es de dimensión `(N, T, q)`, donde: \n",
    "1. `N` Es la cantidad de individuos\n",
    "2. `T` Es la cantidad de puntos en el tiempo (el horizonte de pronóstico)\n",
    "3. `q` corresponde a la cantidad de cuantiles estimados\n",
    "\n",
    "\n",
    "Utiliza las siguientes funciones para completar el código: \n",
    "- ``tf.math.subtract``: https://www.tensorflow.org/api_docs/python/tf/math/subtract\n",
    "- ``tf.math.multiply``: https://www.tensorflow.org/api_docs/python/tf/math/multiply\n",
    "- ``tf.math.maximum``: https://www.tensorflow.org/api_docs/python/tf/math/maximum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class QuantileLoss(Loss):\n",
    "    \n",
    "    def __init__(self, percentiles = [0.1, 0.5, 0.9]):\n",
    "        super().__init__()\n",
    "        self.percentiles = percentiles\n",
    "        self.n_quantiles = len(self.percentiles)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the Quantile Loss\n",
    "        \"\"\"\n",
    "        \n",
    "        percentiles = tf.constant(self.percentiles, shape = [1, self.n_quantiles], dtype = float)\n",
    "        \n",
    "        # Redimensionamos el vector objetivo para repetir sus columnas por cada cuantil a estimar\n",
    "        y_true = tf.expand_dims(y_true, -1)\n",
    "        y_true = tf.repeat(y_true, repeats = self.n_quantiles, axis=-1)\n",
    "        \n",
    "        \n",
    "        ### TU CÓDIGO ###\n",
    "        # Calcula el error de predicción (y - yhat)\n",
    "        error = \n",
    "        \n",
    "        ### TERMINA ###\n",
    "        \n",
    "        under_prediction_error = tf.math.multiply(percentiles, error) # Cálculo de primer término\n",
    "        \n",
    "        ### TU CÓDIGO ###\n",
    "        # Calcula el segundo término de error\n",
    "        over_prediction_error = \n",
    "        \n",
    "        ### TERMINA ###\n",
    "        \n",
    "        \n",
    "        ### TU CÓDIGO ###\n",
    "        # Calcula la pérdida utilizando el máximo\n",
    "        Quantile_Loss = \n",
    "        \n",
    "        ### TERMINA ###\n",
    "        \n",
    "        return tf.math.reduce_mean(Quantile_Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-richmond",
   "metadata": {},
   "source": [
    "¡Verifica tu resultado!\n",
    "\n",
    "Debes obtener:\n",
    "\n",
    "`<tf.Tensor: shape=(), dtype=float32, numpy=1.2333332>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([1, 2, 3], dtype = float)\n",
    "y_pred =  tf.constant([4, 5, 6], dtype = float)\n",
    "\n",
    "QL = QuantileLoss()\n",
    "QL.call(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-ukraine",
   "metadata": {},
   "source": [
    "# Entrenamiento de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_features, path_to_labels):\n",
    "    features = np.load(path_to_features)\n",
    "    labels = np.load(path_to_labels)\n",
    "    \n",
    "    assert features.shape[0] == labels.shape[0]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(256)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = os.path.join('data', 'input_train.npy')\n",
    "train_labels = os.path.join('data', 'output_train.npy')\n",
    "train_dataset = load_dataset(train_features, train_labels)\n",
    "\n",
    "val_features = os.path.join('data', 'input_val.npy')\n",
    "val_labels = os.path.join('data', 'output_val.npy')\n",
    "val_dataset = load_dataset(val_features, val_labels)\n",
    "\n",
    "test_features = os.path.join('data', 'input_test.npy')\n",
    "test_labels = os.path.join('data', 'output_test.npy')\n",
    "test_dataset = load_dataset(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_inf(arrays):\n",
    "    for array in arrays:\n",
    "        loaded_array = np.load(os.path.join(array))\n",
    "        n_inf = np.isinf(loaded_array).sum()\n",
    "        if n_inf:\n",
    "            print(f'Array {array} contains INF values')\n",
    "            \n",
    "        n_nan = np.isnan(loaded_array).sum()\n",
    "        if n_nan:\n",
    "            print(f'Array {array} contains NAN values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-cartoon",
   "metadata": {},
   "source": [
    "¡Listo! Podremos realizar el entrenamiento de nuestro modelo. Para poderlo hacer necesitarás definir dos cosas:\n",
    "\n",
    "1. ¿Qué percentiles utilizarás? Por defecto tenemos el percentil 10%, 50% y 90%, pero puedes definir otros diferentes.\n",
    "2. ¡Claramente la arquitectura de red!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_STEPS = 24\n",
    "\n",
    "### TU CÓDIGO AQUÍ ###\n",
    "\n",
    "# Define los percentiles que quieres estimar en una lista (e.g. [0.1, 0.5, 0.9])\n",
    "PERCENTILES = \n",
    "\n",
    "### TERMINA ###\n",
    "\n",
    "percentiles = len(PERCENTILES)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "                ### TU CÓDIGO AQUÍ ###\n",
    "                # Define la arquitectura de red que quieras utilizar, no definas la capa de salida\n",
    "    \n",
    "              \n",
    "                ### TERMINA ###\n",
    "                tf.keras.layers.Dense(OUT_STEPS*percentiles, kernel_initializer=tf.initializers.zeros()),\n",
    "                tf.keras.layers.Reshape((OUT_STEPS, percentiles))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_folder = 'training/'\n",
    "tf_log_dir = 'training/logs'\n",
    "\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.mkdir(checkpoint_folder)\n",
    "    \n",
    "if not os.path.exists(tf_log_dir):\n",
    "    os.mkdir(tf_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'pfcst'\n",
    "execution_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tf_log_dir = 'training'\n",
    "logdir = f\"{tf_log_dir}/probabilistic/{name}_{execution_time}\" \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "checkpoint_path = f\"{checkpoint_folder}/checkpoints_{name}_{execution_time}/cp.ckpt\"\n",
    "checkpoints_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "### TU CÓDIGO AQUÍ\n",
    "# Define tu Early Stopping Callback\n",
    "\n",
    "earlystopping_callback = \n",
    "\n",
    "### TERMINA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TU CÓDIGO AQUÍ\n",
    "# Define el otpimizador que vas a utilizar y sus parámetros\n",
    "optimizer = \n",
    "\n",
    "### TERMINA \n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss=QuantileLoss(PERCENTILES),# tf.losses.MeanSquaredError(),\n",
    "     \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-space",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          callbacks=[tensorboard_callback, checkpoints_callback, earlystopping_callback],\n",
    "          epochs=EPOCHS, \n",
    "          verbose =1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-bottom",
   "metadata": {},
   "source": [
    "Para este punto ya tenemos un modelo entrenado, y el mejor dada la configuración se encuentra almacenado en `checkpoint_path`. A continuación vamos a observar sus resultados y hacer una pequeña evaluación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-banana",
   "metadata": {},
   "source": [
    "# Análisis de Desempeño\n",
    "\n",
    "Carga los datos de prueba y realiza la predicción del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(test_features)\n",
    "Y = np.load(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TU CÓDIGO AQUÍ\n",
    "# Realiza la predicción del modelo\n",
    "Y_hat = \n",
    "\n",
    "### TERMINA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-guatemala",
   "metadata": {},
   "source": [
    "A continuación, deberás definir la función que graficará tus resultados. Ten en cuenta que debes realizar las transformaciones inversas a las que aplicaste en la generación de ventanas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(Y_past, Y_true, Y_pred, idx_obs, exponent = True):\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    input_length = Y_past.shape[1]\n",
    "    ouput_length = Y_pred.shape[1]\n",
    "\n",
    "    plt.plot(np.arange(input_length), Y_past[idx_obs, :, 0], color = 'black')\n",
    "\n",
    "    ### TU CÓDIGO AQUÍ ###\n",
    "    # Grafica la serie objetivo y la serie predicha para un idx_obs particular\n",
    "\n",
    "    \n",
    "    \n",
    "    ### TERMINA ###\n",
    "\n",
    "    \n",
    "    # Asumimos que el cuantil más bajo es el primero y el más alto es el último\n",
    "    plt.fill_between(np.arange(input_length, input_length+ouput_length), Y_pred[idx_obs, :, 0], Y_pred[idx_obs, :, -1], color='b', alpha=.1)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(X, Y, Y_hat, 3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-overhead",
   "metadata": {},
   "source": [
    "Ya que tenemos una función para graficar nuestras predicciones, vamos a hacer el cálculo del MAPE de cada serie y del modelo en general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(Y_true, Y_pred, return_list = True):\n",
    "\n",
    "    Y_true = Y_true + 1\n",
    "    ### TU CÓDIGO AQUÍ ###\n",
    "    # Escoge el quantil sobre el cual vas a hacer la comparación (principalmente sería la mediana)\n",
    "    Y_pred = Y_pred[:, :, 1]\n",
    "        \n",
    "    ### TERMINA\n",
    "    \n",
    "    Y_pred = Y_pred.clip(min = 1)\n",
    "    \n",
    "    ### TU CÓDIGO AQUÍ ###\n",
    "    # Calcula el MAPE para cada serie\n",
    "    \n",
    "    \n",
    "    MAPE_customer =  # Una lista de MAPE de dimensión Y_true.shape[0]\n",
    "    \n",
    "    ### TERMINA ###\n",
    "    print('Overall model MAPE: {:.2%}'.format(np.mean(MAPE_customer)))\n",
    "    print('Overall model MAPE (median): {:.2%}'.format(np.median(MAPE_customer)))\n",
    "    return MAPE_customer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranked(Y_true, Y_pred, X, mape, best = True, top = 5, exponent = True):\n",
    "    sorted_mape = np.argsort(mape)\n",
    "    if best:\n",
    "        idx = sorted_mape[:top]\n",
    "    else:\n",
    "        idx = sorted_mape[-top:]\n",
    "    for i in idx:\n",
    "  \n",
    "        plot_predictions(X, Y_true, Y_pred, i, exponent)\n",
    "        plt.title(\"{:.2%}\".format(mape[i]))\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_list = compute_mape(Y, Y_hat, exponent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ranked(Y, Y_hat, X, mape_list, exponent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ranked(Y, Y_hat, X, mape_list, exponent = False, best = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
